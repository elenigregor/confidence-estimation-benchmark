{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "84f5727f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import yaml\n",
    "import os.path\n",
    "from os import path\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3b0b30",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d9ed7e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "TEST_EXAMPLE_PERCENTAGE = 0.33\n",
    "VERSION = 8\n",
    "NUM_SPLITS = 10\n",
    "\n",
    "# paths\n",
    "DATASET_FILE = '../datasets/nlu-home-domain.csv'\n",
    "\n",
    "# lists\n",
    "NLU_NAMES = ['watson', 'luis', 'snips', 'rasa-sklearn', 'rasa-diet']\n",
    "INTENTS = ['factoid', 'music', 'negate', 'query', 'quirky', 'remove', 'set', 'sendemail', 'repeat', 'explain']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0c8a8415",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>intent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>wake me up at 5am this week</td>\n",
       "      <td>set</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>wake me up at 9am on Friday</td>\n",
       "      <td>set</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>set an alarm for two hours from now</td>\n",
       "      <td>set</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>check when the show starts</td>\n",
       "      <td>query</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>I WANT TO LISTEN ARIJIT SINGH SONG ONCE AGAIN.</td>\n",
       "      <td>music</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24926</th>\n",
       "      <td>s2, i couldn't catch that.</td>\n",
       "      <td>explain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24927</th>\n",
       "      <td>what do you mean by it.</td>\n",
       "      <td>explain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24928</th>\n",
       "      <td>can you clarify more on that.</td>\n",
       "      <td>explain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24929</th>\n",
       "      <td>s1, rephrase me more on it.</td>\n",
       "      <td>explain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24930</th>\n",
       "      <td>s2, i couldn't catch up with you.</td>\n",
       "      <td>explain</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14962 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text   intent\n",
       "0                         wake me up at 5am this week      set\n",
       "1                         wake me up at 9am on Friday      set\n",
       "2                 set an alarm for two hours from now      set\n",
       "24                         check when the show starts    query\n",
       "25     I WANT TO LISTEN ARIJIT SINGH SONG ONCE AGAIN.    music\n",
       "...                                               ...      ...\n",
       "24926                      s2, i couldn't catch that.  explain\n",
       "24927                         what do you mean by it.  explain\n",
       "24928                   can you clarify more on that.  explain\n",
       "24929                     s1, rephrase me more on it.  explain\n",
       "24930               s2, i couldn't catch up with you.  explain\n",
       "\n",
       "[14962 rows x 2 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(DATASET_FILE, delimiter=';', usecols=lambda x: x.lower() in ['intent', 'scenario', 'answer'])\n",
    "\n",
    "data = data.loc[data['intent'].isin(INTENTS)]\n",
    "    \n",
    "data = data[['answer', 'intent']]\n",
    "data = data.rename(columns={\"answer\": \"text\"})\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "738784d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['set', 'query', 'music', 'quirky', 'remove', 'sendemail',\n",
       "       'factoid', 'negate', 'repeat', 'explain'], dtype=object)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_intents = data['intent'].unique()\n",
    "selected_intents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45457316",
   "metadata": {},
   "source": [
    "### classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fb53baa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class literal_unicode(str): pass\n",
    "\n",
    "def literal_unicode_representer(dumper, data):\n",
    "    return dumper.represent_scalar(u'tag:yaml.org,2002:str', data, style='|')\n",
    "\n",
    "yaml.add_representer(literal_unicode, literal_unicode_representer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf80a1b2",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "eb446751",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_csv(data, output_csv):\n",
    "    with open(output_csv, 'w') as f: \n",
    "        write = csv.writer(f) \n",
    "        write.writerows(data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "89396d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_json(data, output_file):\n",
    "    with open(output_file, 'w') as outfile:\n",
    "        json.dump(data, outfile, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e883c6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_yaml(data, output_yaml):\n",
    "    with open(output_yaml, 'w') as f:\n",
    "        yaml.dump(data, f, sort_keys=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1c8508b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_suffix(nlu):\n",
    "    if nlu=='snips' or nlu=='luis':\n",
    "        return '.json'\n",
    "    elif nlu=='watson':\n",
    "        return '.csv'\n",
    "    elif 'rasa' in nlu:\n",
    "        return '.yml'\n",
    "    else:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fc102341",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_params(nlu_name, split_num, version=VERSION):\n",
    "    suffix = get_file_suffix(nlu_name)\n",
    "    # paths\n",
    "    output_train_file = '../datasets/' + nlu_name + '/v' + str(version) + '/' + nlu_name +'_split_' + str(split_num) + '_train_v' + str(version) + suffix\n",
    "    output_test_file = '../datasets/' + nlu_name + '/v' + str(version) + '/' + nlu_name + '_split_' + str(split_num) + '_test_v' + str(version) + '.csv'\n",
    "    return output_train_file, output_test_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "54fd67d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(utterances, nlu='snips'):\n",
    "    # shuffle examples\n",
    "    utterances = utterances.sample(frac=1).reset_index(drop=True)\n",
    "    # drop NaN and None values\n",
    "    utterances = utterances.loc[utterances['text'] != None]\n",
    "    utterances = utterances.dropna()\n",
    "    # clean text\n",
    "#     utterances['text'] = utterances['text'].str.replace(r'[^\\w\\s]+', '')\n",
    "#     utterances['text'] = utterances['text'].str.lower()\n",
    "    if 'rasa' in nlu:\n",
    "        # remove ascii chars\n",
    "        utterances['text'] = utterances.apply(lambda utt : bytes(utt[0], 'utf-8').decode('ascii', 'ignore'), axis = 1)\n",
    "    return utterances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "67d7d850",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_utterance_json_body(utterance):\n",
    "    utterance_body = { \"data\": [{ \"text\": utterance }] }\n",
    "    return utterance_body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2da36e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_intent_json_body(intent_examples):\n",
    "    # utterances body\n",
    "    utterances = []\n",
    "    for example in intent_examples:\n",
    "        utterance_body = get_utterance_json_body(example)\n",
    "        utterances.append(utterance_body)\n",
    "        \n",
    "    # intent body\n",
    "    intent = {\"utterances\": utterances}\n",
    "    return intent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7b01a090",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_intent_yaml_body(intent_examples):\n",
    "    intent_examples = intent_examples['text'].values.tolist()\n",
    "    # prep data for train file .yml\n",
    "    examples = \"\"\n",
    "    for example in intent_examples:\n",
    "        manipulated_example = \"- \" + str(example) + \"\\n\"\n",
    "        examples = examples + manipulated_example\n",
    "    intent_obj = {'intent': intent, 'examples': literal_unicode(examples)}\n",
    "    return intent_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c578d875",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_filtered_data(nlu='snips'):\n",
    "    if nlu=='snips':\n",
    "        return {}\n",
    "    elif nlu=='watson' or nlu=='luis':\n",
    "        return pd.DataFrame([], columns = ['text', 'intent'])\n",
    "    elif 'rasa' in nlu:\n",
    "        return {\"nlu\" : []}\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d24cfc53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_filtered_data(filtered_data, intent, intent_examples, nlu='snips'):\n",
    "    if nlu=='snips':\n",
    "        intent_examples = intent_examples['text'].values.tolist()\n",
    "        filtered_data[intent] = get_intent_json_body(intent_examples)\n",
    "    elif nlu=='watson' or nlu=='luis':\n",
    "        filtered_data = pd.concat([filtered_data, intent_examples], ignore_index=True, sort=False)\n",
    "    elif 'rasa' in nlu:\n",
    "        intent_obj = get_intent_yaml_body(intent_examples)\n",
    "        filtered_data[\"nlu\"].append(intent_obj)\n",
    "    \n",
    "    else:\n",
    "        pass\n",
    "    return filtered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "431f7a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_luis_json(filtered_data, selected_intents):\n",
    "    filtered_data['entities'] = [[]] * filtered_data.shape[0]\n",
    "    result = filtered_data.to_json(orient=\"records\")\n",
    "    parsed = json.loads(result)\n",
    "\n",
    "    intents = [{\"name\" : intent, \"features\" : []} for intent in selected_intents]\n",
    "    json_data = {\"luis_schema_version\": \"6.0.0\",\n",
    "      \"intents\": intents,\n",
    "      \"entities\": [],\n",
    "      \"hierarchicals\": [],\n",
    "      \"composites\": [],\n",
    "      \"closedLists\": [],\n",
    "      \"prebuiltEntities\": [],\n",
    "      \"utterances\": parsed,\n",
    "      \"versionId\": \"0.1\",\n",
    "      \"name\": \"HomeAssistant\",\n",
    "      \"desc\": \"\",\n",
    "      \"culture\": \"en-us\",\n",
    "      \"tokenizerVersion\": \"1.0.0\",\n",
    "      \"patternAnyEntities\": [],\n",
    "      \"regex_entities\": [],\n",
    "      \"phraselists\": [],\n",
    "      \"regex_features\": [],\n",
    "      \"patterns\": [],\n",
    "      \"settings\": [\n",
    "        {\n",
    "          \"name\": \"UseAllTrainingData\",\n",
    "          \"value\": \"false\"\n",
    "        }]}\n",
    "    return json_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bc6c4aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_snips_json(filtered_data):\n",
    "    json_data = {\n",
    "            \"entities\": {},\n",
    "            \"intents\": filtered_data,\n",
    "            \"language\": \"en\"\n",
    "        }\n",
    "    return json_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b7e3fdbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_training_data(filtered_data, output_file, nlu, selected_intents):\n",
    "    if nlu=='snips':\n",
    "        json_data = build_snips_json(filtered_data)\n",
    "        write_to_json(json_data, output_file)\n",
    "        \n",
    "    elif nlu=='luis':\n",
    "        json_data = build_luis_json(filtered_data, selected_intents)\n",
    "        write_to_json(json_data, output_file)\n",
    "        \n",
    "    elif nlu=='watson':\n",
    "        filtered_data.to_csv(output_file, index=False, header=False)\n",
    "        \n",
    "    elif 'rasa' in nlu:\n",
    "        write_to_yaml(filtered_data, output_file)\n",
    "        \n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e389f06",
   "metadata": {},
   "source": [
    "### Re-structure dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "53746be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for split_num in range(NUM_SPLITS):\n",
    "    seed = random.randint(0, 42)\n",
    "    for nlu in NLU_NAMES:\n",
    "        train_data, test_data = train_test_split(data, stratify=data['intent'], test_size=TEST_EXAMPLE_PERCENTAGE, random_state=seed)\n",
    "        filtered_data = initialize_filtered_data(nlu=nlu)\n",
    "\n",
    "        for intent in selected_intents:\n",
    "            # get all examples for this intent\n",
    "            intent_examples = train_data.loc[train_data['intent'] == intent]\n",
    "            intent_examples = preprocess(intent_examples, nlu=nlu)\n",
    "\n",
    "            filtered_data = update_filtered_data(filtered_data, intent, intent_examples, nlu=nlu)\n",
    "            \n",
    "        output_train_file, output_test_file = get_params(nlu, (split_num + 1))\n",
    "        if not path.exists('../datasets/' + nlu):\n",
    "            os.mkdir('../datasets/' + nlu)\n",
    "        if not path.exists('../datasets/' + nlu + '/v' + str(VERSION)):\n",
    "            os.mkdir('../datasets/' + nlu + '/v' + str(VERSION))\n",
    "        write_training_data(filtered_data, output_train_file, nlu, selected_intents)\n",
    "        test_data.to_csv(output_test_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaea062d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
